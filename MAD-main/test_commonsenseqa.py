#!/usr/bin/env python3
"""
Test script for CommonSenseQA dataset
"""
import json
from datasets import load_dataset
from src.debate.models import LLMFactory
from src.debate.prompts import parse_json_or_fallback, parse_judge_json

def test_commonsenseqa():
    """Test CommonSenseQA dataset"""
    
    print("=== CommonSenseQA Test ===")
    
    # Load dataset
    dataset = load_dataset('commonsense_qa', split='validation')
    example = dataset[0]
    
    print(f"Question: {example['question']}")
    print(f"Choices: {example['choices']}")
    print(f"Answer: {example['answerKey']}")
    print(f"Question length: {len(example['question'])} characters")
    
    # Convert to our format
    choices_dict = {}
    text_list = example['choices']['text']
    label_list = example['choices']['label']
    for i, label in enumerate(label_list):
        if i < len(text_list):
            choices_dict[label] = text_list[i]
    
    choices_csv = ", ".join([f"{k}: {v}" for k, v in choices_dict.items()])
    
    # Test debater
    print("\n--- Testing Debater ---")
    debater = LLMFactory.make("local", "Qwen/Qwen2.5-7B-Instruct", temperature=0.7, max_tokens=1024)
    
    debater_prompt = f"""Round 1. You are presented with the following multiple-choice question, and provide your own analysis of output and reasoning:

Question: {example['question']}
Choices: {choices_csv}

Output (strict JSON):
{{"output": {{"A": pA, "B": pB, "C": pC, "D": pD, "E": pE}}, "reason": {{"A": rA, "B": rB, "C": rC, "D": rD, "E": rE}}}}"""

    try:
        messages = [
            {"role": "system", "content": "You are a careful multiple-choice reasoner. Always answer in STRICT JSON and nothing else."},
            {"role": "user", "content": debater_prompt}
        ]
        
        response = debater.invoke(messages)
        parsed = parse_json_or_fallback(response.content, list(choices_dict.keys()))
        
        print(f"âœ… Debater response parsed successfully")
        print(f"   Probabilities: {parsed['probs']}")
        print(f"   Top choice: {max(parsed['probs'], key=parsed['probs'].get)}")
        
        debater_success = True
    except Exception as e:
        print(f"âŒ Debater failed: {e}")
        debater_success = False
    
    # Test judge
    print("\n--- Testing Judge ---")
    judge = LLMFactory.make("local", "Qwen/Qwen2.5-7B-Instruct", temperature=0.2, max_tokens=2048)
    
    judge_prompt = f"""Round: 1
Question: {example['question']}
Choices: {choices_csv}

# Agent A (Round 1)
outputA: {{"A": 0.2, "B": 0.2, "C": 0.2, "D": 0.2, "E": 0.2}}
reasonA: {{"A": "Equal probability", "B": "Equal probability", "C": "Equal probability", "D": "Equal probability", "E": "Equal probability"}}

# Agent B (Round 1)
outputB: {{"A": 0.1, "B": 0.2, "C": 0.4, "D": 0.2, "E": 0.1}}
reasonB: {{"A": "Less likely", "B": "Somewhat likely", "C": "Most likely", "D": "Somewhat likely", "E": "Less likely"}}

CRIT: "Function Î“ = CRIT(d)
Input: document d   Output: validation score Î“
Vars: Î© claim; R and Râ€² sets of reasons and rival reasons
Subs: CLAIM(), FINDDOC(), VALIDATE()
Begin
#1â€“#2 Identify in d the claim Î©. Find a set of supporting reasons R for Î©.
#3 For each r âˆˆ R evaluate r â‡’ Î©.
   If CLAIM(r) then (Î³_r, Î¸_r) = CRIT(FINDDOC(r)).
   Else (Î³_r, Î¸_r) = VALIDATE(r â‡’ Î©).
#4â€“#6â€“#7â€“#8 Find a set of rival reasons Râ€² against Î©.
   #5 For each râ€² âˆˆ Râ€² compute (Î³_râ€², Î¸_râ€²) = VALIDATE(râ€² â‡’ Î©).
   Compute a weighted sum Î“ from the validation scores.
   Analyze arguments to justify the final Î“ score.
   Reflect on transfer of CRIT to other contexts.
End"

Output STRICT JSON only:
{{
  "outputA": {{"A": pA, "B": pB, "C": pC, "D": pD, "E": pE}},
  "outputB": {{"A": pA, "B": pB, "C": pC, "D": pD, "E": pE}},
  "CRIT_A": float,
  "CRIT_B": float,
  "NOTE_A": "string",
  "NOTE_B": "string"
}}"""

    try:
        messages = [
            {"role": "system", "content": "You are a rigorous, deterministic judge. Apply the CRIT algorithm directly and output STRICT JSON only."},
            {"role": "user", "content": judge_prompt}
        ]
        
        response = judge.invoke(messages)
        parsed = parse_judge_json(response.content, list(choices_dict.keys()))
        
        if parsed["CRIT_A"] is not None and parsed["CRIT_B"] is not None:
            print(f"âœ… Judge response parsed successfully")
            print(f"   CRIT_A: {parsed['CRIT_A']}")
            print(f"   CRIT_B: {parsed['CRIT_B']}")
            judge_success = True
        else:
            print(f"âŒ Judge returned null CRIT scores")
            judge_success = False
            
    except Exception as e:
        print(f"âŒ Judge failed: {e}")
        judge_success = False
    
    print(f"\n=== CommonSenseQA Test Results ===")
    print(f"Debater: {'âœ… PASS' if debater_success else 'âŒ FAIL'}")
    print(f"Judge: {'âœ… PASS' if judge_success else 'âŒ FAIL'}")
    
    return debater_success and judge_success

if __name__ == "__main__":
    success = test_commonsenseqa()
    if success:
        print("ðŸŽ‰ CommonSenseQA test passed!")
    else:
        print("âš ï¸ CommonSenseQA test failed!")
